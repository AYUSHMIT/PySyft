{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syft absolute\n",
    "import syft as sy\n",
    "from syft.service.log.log import SyftLogV3\n",
    "from syft.types.blob_storage import BlobStorageEntry\n",
    "from syft.types.blob_storage import CreateBlobStorageEntry\n",
    "from syft.types.syft_object import Context\n",
    "from syft.types.syft_object import SyftObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"syft version: {sy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "TODOS\n",
    "- [x] action objects\n",
    "- [x] maybe an example of how to migrate one object type in a custom way\n",
    "- [x] check SyftObjectRegistry and compare with current implementation\n",
    "- [x] run unit tests\n",
    "- [ ] finalize notebooks for testing, run in CI\n",
    "- [ ] other tasks defined in tickets\n",
    "- [ ] also get actionobjects in get_migration_objects\n",
    "- [ ] make clientside method to migrate blobstorage `migrate_blobstorage(from_client, to_client, migration_data)`\n",
    "- [ ] make domainclient get_migration and apply_migration (to/from file?)\n",
    "    - merge with save_migration_objects_to_file / migrate_objects_from_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = sy.orchestra.launch(\n",
    "    name=\"test_upgradability\",\n",
    "    dev_mode=True,\n",
    "    local_db=True,\n",
    "    n_consumers=2,\n",
    "    create_producer=True,\n",
    "    migrate=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = node.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Client side migrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_node = sy.orchestra.launch(\n",
    "    name=\"temp_node\",\n",
    "    dev_mode=True,\n",
    "    local_db=True,\n",
    "    n_consumers=2,\n",
    "    create_producer=True,\n",
    "    migrate=False,\n",
    "    reset=True,\n",
    ")\n",
    "\n",
    "temp_client = temp_node.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_data = client.services.migration.get_migration_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syft absolute\n",
    "from syft.client.migrations import migrate_blob_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sy.migrate(from_client=client, to_client=temp_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "migrate_blob_storage(\n",
    "    from_client=client,\n",
    "    to_client=temp_client,\n",
    "    blob_storage_objects=migration_data.blob_storage_objects,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_data.store_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_client.api.services.migration.apply_migration_data(migration_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## document store objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_dict = client.services.migration.get_migration_objects(get_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_migration_function(context, obj: SyftObject, klass) -> SyftObject:\n",
    "    # Here, we are just doing the same, but this is where you would write your custom logic\n",
    "    return obj.migrate_to(klass.__version__, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this wont work in the cases where the context is actually used,\n",
    "# but since this would need custom logic here anyway you write workarounds for that (manually querying required state)\n",
    "\n",
    "\n",
    "context = Context()\n",
    "migrated_objects = []\n",
    "for klass, objects in migration_dict.items():\n",
    "    for obj in objects:\n",
    "        if isinstance(obj, BlobStorageEntry):\n",
    "            continue\n",
    "        elif isinstance(obj, SyftLogV3):\n",
    "            migrated_obj = custom_migration_function(context, obj, klass)\n",
    "        else:\n",
    "            try:\n",
    "                migrated_obj = obj.migrate_to(klass.__version__, context)\n",
    "            except Exception:\n",
    "                print(obj.__version__, obj.__canonical_name__)\n",
    "                print(klass.__version__, klass.__canonical_name__)\n",
    "                raise\n",
    "\n",
    "        migrated_objects.append(migrated_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO what to do with workerpools\n",
    "# TODO what to do with admin? @yash: can we make new node with existing verifykey?\n",
    "# TODO check asset AO is not saved in blobstorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = temp_client.services.migration.create_migrated_objects(migrated_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(res, sy.SyftSuccess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Migrate blobstorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "klass = BlobStorageEntry\n",
    "blob_entries = migration_dict[klass]\n",
    "obj = blob_entries[0]\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from io import BytesIO\n",
    "import sys\n",
    "\n",
    "\n",
    "def migrate_blob_entry_data(\n",
    "    old_client, new_client, obj, klass\n",
    ") -> sy.SyftSuccess | sy.SyftError:\n",
    "    migrated_obj = obj.migrate_to(klass.__version__, Context())\n",
    "    uploaded_by = migrated_obj.uploaded_by\n",
    "    blob_retrieval = old_client.services.blob_storage.read(obj.id)\n",
    "    if isinstance(blob_retrieval, sy.SyftError):\n",
    "        return blob_retrieval\n",
    "\n",
    "    data = blob_retrieval.read()\n",
    "    # TODO do we have to determine new filesize here?\n",
    "    serialized = sy.serialize(data, to_bytes=True)\n",
    "    size = sys.getsizeof(serialized)\n",
    "    blob_create = CreateBlobStorageEntry.from_blob_storage_entry(obj)\n",
    "    blob_create.file_size = size\n",
    "\n",
    "    blob_deposit_object = new_client.services.blob_storage.allocate_for_user(\n",
    "        blob_create, uploaded_by\n",
    "    )\n",
    "    if isinstance(blob_deposit_object, sy.SyftError):\n",
    "        return blob_deposit_object\n",
    "    return blob_deposit_object.write(BytesIO(serialized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for blob_entry in blob_entries:\n",
    "    res = migrate_blob_entry_data(client, temp_client, blob_entry, BlobStorageEntry)\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.services.blob_storage.get_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Actions and ActionObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_action_dict = client.services.migration.get_migration_actionobjects(\n",
    "    get_all=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ao = migration_action_dict[list(migration_action_dict.keys())[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ao.syft_action_data_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "node.python_node.action_store.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.jobs[0].result.id.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "node.python_node.action_store.data[sy.UID(\"106b561961c74a46afc63c5c73c24212\")].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this wont work in the cases where the context is actually used, but since this you would need custom logic here anyway\n",
    "# it doesnt matter\n",
    "context = Context()\n",
    "migrated_actionobjects = []\n",
    "for klass, objects in migration_action_dict.items():\n",
    "    for obj in objects:\n",
    "        # custom migration logic here\n",
    "        migrated_actionobject = obj.migrate_to(klass.__version__, context)\n",
    "        migrated_actionobjects.append(migrated_actionobject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(migrated_actionobjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = temp_client.services.migration.update_migrated_actionobjects(\n",
    "    migrated_actionobjects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(res, sy.SyftSuccess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in temp_node.python_node.action_store.data:\n",
    "    ao = temp_client.services.action.get(uid)\n",
    "    ao.reload_cache()\n",
    "    print(ao.syft_action_data_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in node.python_node.action_store.data:\n",
    "    ao = client.services.action.get(uid)\n",
    "    ao.reload_cache()\n",
    "    print(ao.syft_action_data_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Store metadata\n",
    "\n",
    "- Permissions\n",
    "- StoragePermissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_metadata = client.services.migration.get_all_store_metadata()\n",
    "store_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in store_metadata.items():\n",
    "    if len(v.permissions):\n",
    "        print(\n",
    "            k, len(v.permissions), len(v.permissions) == len(migration_dict.get(k, []))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test update method with a temp node\n",
    "# After update, all metadata should match between the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_client.services.migration.update_store_metadata(store_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cname, real_partition in node.python_node.document_store.partitions.items():\n",
    "    temp_partition = temp_node.python_node.document_store.partitions[cname]\n",
    "\n",
    "    temp_perms = dict(temp_partition.permissions.items())\n",
    "    real_perms = dict(real_partition.permissions.items())\n",
    "\n",
    "    for k, temp_v in temp_perms.items():\n",
    "        if k not in real_perms:\n",
    "            continue\n",
    "        real_v = real_perms[k]\n",
    "        assert real_v.issubset(temp_v)\n",
    "\n",
    "    temp_storage = dict(temp_partition.storage_permissions.items())\n",
    "    real_storage = dict(real_partition.storage_permissions.items())\n",
    "    for k, temp_v in temp_storage.items():\n",
    "        if k not in real_storage:\n",
    "            continue\n",
    "        real_v = real_storage[k]\n",
    "        assert real_v.issubset(temp_v)\n",
    "\n",
    "# Action store\n",
    "real_partition = node.python_node.action_store\n",
    "temp_partition = temp_node.python_node.action_store\n",
    "temp_perms = dict(temp_partition.permissions.items())\n",
    "real_perms = dict(real_partition.permissions.items())\n",
    "\n",
    "# Only look at migrated items\n",
    "temp_perms = {k: v for k, v in temp_perms.items() if k in real_perms}\n",
    "for k, temp_v in temp_perms.items():\n",
    "    if k not in real_perms:\n",
    "        continue\n",
    "    real_v = real_perms[k]\n",
    "    assert real_v.issubset(temp_v)\n",
    "\n",
    "temp_storage = dict(temp_partition.storage_permissions.items())\n",
    "real_storage = dict(real_partition.storage_permissions.items())\n",
    "for k, temp_v in temp_storage.items():\n",
    "    if k not in real_storage:\n",
    "        continue\n",
    "    real_v = real_storage[k]\n",
    "    assert real_v.issubset(temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
