{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kj/filesystem-disk-unix.c++:1703: warning: PWD environment variable doesn't match current directory; pwd = /home/teo/OpenMined/PySyft\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "from syft.store.blob_storage import BlobStorageConfig, BlobStorageClientConfig\n",
    "from syft.store.blob_storage.seaweedfs import SeaweedFSClient, SeaweedFSClientConfig\n",
    "from syft import ActionObject\n",
    "from syft.service.action.action_data_empty import ActionFileData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING A PRODUCER ON tcp://127.0.0.1:34467\n",
      "CREATING A CONSUMER ON tcp://127.0.0.1:36427\n",
      "spawning thread\n",
      "CREATING A CONSUMER ON tcp://127.0.0.1:36427\n",
      "spawning thread\n",
      "CREATING A CONSUMER ON tcp://127.0.0.1:36427\n",
      "spawning thread\n",
      "CREATING A CONSUMER ON tcp://127.0.0.1:36427\n",
      "spawning thread\n",
      "CREATING A CONSUMER ON tcp://127.0.0.1:36427\n",
      "spawning thread\n",
      "CREATING A CONSUMER ON tcp://127.0.0.1:36427\n",
      "spawning thread\n",
      "Logged into <test-domain-helm2: High side Domain> as <info@openmined.org>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert-warning\" style=\"padding:5px;\"><strong>SyftWarning</strong>: You are using a default password. Please change the password using `[your_client].me.set_password([new_password])`.</div><br />"
      ],
      "text/plain": [
       "SyftWarning: You are using a default password. Please change the password using `[your_client].me.set_password([new_password])`."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "node = sy.orchestra.launch(name=\"test-domain-helm2\", dev_mode=True, reset=True, n_consumers=6)\n",
    "client = node.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run --entrypoint /bin/sh -p 8333:8333 -p 8888:8888 chrislusf/seaweedfs -c \"echo 's3.configure -access_key admin -secret_key admin -user iam -actions Read,Write,List,Tagging,Admin -apply' | weed shell > /dev/null 2>&1 & weed server -s3 -s3.port=8333 -master.volumeSizeLimitMB=2048\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_config = BlobStorageConfig(client_type=SeaweedFSClient,\n",
    "                                client_config=SeaweedFSClientConfig(host=\"http://0.0.0.0\",\n",
    "                                                                    port=\"8333\",\n",
    "                                                                    access_key=\"admin\",\n",
    "                                                                    secret_key=\"admin\",\n",
    "                                                                    bucket_name=\"test_bucket\",\n",
    "                                                                    region=\"us-east-1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.python_node.init_blob_storage(blob_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_obj = sy.ActionObject.from_path(path=\"/home/teo/helm/scripts/data_overlap/scenario_data.jsonl\")\n",
    "scenario_ptr = scenario_obj.send(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_obj = sy.ActionObject.from_path(\"short_input.jsonl\")\n",
    "input_ptr = input_obj.send(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"text\":\"\\\\nChina Deserves Donald Trump - rm2889\\\\nhttps:\\\\/\\\\/www.nytimes.com\\\\/2019\\\\/05\\\\/21\\\\/opinion\\\\/china-trump-trade.html\\\\n======\\\\nNotPaidToPost\\\\n> so he\\\\u2019d be wise to curb his nationalistic \\\\u201cno-one-tells-China-what-to-do\\\\u201d\\\\n> bluster\\\\n\\\\nThis comment highlights both ignorance of Chinese history and continuing\\\\nAmerican arrogance.\\\\n\\\\nChina has been painfully dictated what to do during the last 200 years. This\\\\nhas had a profound effect on the country and has led to the collapse of\\\\nimperial rule and the drive to \\'rejuvenate\\' the country (to use the official\\\\nterm in China).\\\\n\\\\nThis is also arrogant because it suggests that China should be told what to do\\\\ncoming from THE country (the USA) that really is the archetype of \\\\\"no-one\\\\ntells us what to do\\\\\".\\\\n\\\\nI would quip that one of the US\\'s issues with China is that China is not told\\\\nwhat to do and is too big to be easily coerced. A bit of a rude awakening for\\\\nthe US...\\\\n\\\\n> Huawei then uses ... its rising global market dominance to set the next\\\\n> generation of global 5G telecom standards around its own technologies, not\\\\n> those of Qualcomm or Sweden\\\\u2019s Ericsson.\\\\n\\\\nWhich is exactly what Qualcomm did for 3G. Don\\'t hate the player, hate the\\\\ngame.\\\\n\\\\n~~~\\\\nFjolsvith\\\\n>> so he\\\\u2019d be wise to curb his nationalistic \\\\u201cno-one-tells-China-what-to-do\\\\u201d\\\\nbluster >This comment highlights both ignorance of Chinese history and\\\\ncontinuing American arrogance.\\\\n\\\\n>China has been painfully dictated what to do during the last 200 years. This\\\\nhas had a profound effect on the country and has led to the collapse of\\\\nimperial rule and the drive to \\'rejuvenate\\' the country (to use the official\\\\nterm in China).\\\\n\\\\nI disagree. China has been given some unfair advantages in order to help it\\\\nbuild its economy over the last 40 years. Instead of growing up and becoming\\\\nan adult, they\\'ve become the playground bully with their IP theft and closed\\\\nmarket.\\\\n\\\\n>This is also arrogant because it suggests that China should be told what to\\\\ndo coming from THE country (the USA) that really is the archetype of \\\\\"no-one\\\\ntells us what to do\\\\\".\\\\n\\\\nIf China doesn\\'t figure out the game real fast, they\\'re going to lose it. And\\\\nto do that, they really need to do what people are telling them they should\\\\ndo.\\\\n\\\\n~~~\\\\nNotPaidToPost\\\\n> I disagree\\\\n\\\\nI should point that the part of my comment you quoted expressed the historical\\\\nreality, not an opinion.\\\\n\\\\n~~~\\\\nFjolsvith\\\\nI still disagree.\\\\n\\\\n~~~\\\\nNotPaidToPost\\\\nThe good thing with disagreeing with reality is that reality does not care.\\\\n\\\\n------\\\\ncfarm\\\\nThis article makes a good point about \\\\\"cheating\\\\\". I personally don\\'t like that\\\\nword here, but by blocking other companies like Amazon, Google, FB, etc from\\\\nentering in China, then copying those companies and selling the products to\\\\nthe rest of the world, this presents a problem for trade fairness.\\\\n\\\\n\",\"meta\":\"{\\'id\\': \\'19979654\\'}\"}'\n",
      "b'{\"text\":\"\\\\nHow to Be an Effective CEO - terpua\\\\nhttp:\\\\/\\\\/www.readwriteweb.com\\\\/readwritestart\\\\/2009\\\\/07\\\\/how-to-be-an-effective-ceo.php\\\\n======\\\\npclark\\\\nloved this line: \\\\\"Core is what you have to do really well and do in-house.\\\\nEverything else you can and should outsource\\\\\"\\\\n\\\\n\",\"meta\":\"{\\'id\\': \\'685596\\'}\"}'\n",
      "b'{\"text\":\"\\\\nA Survey of Deep Learning for Scientific Discovery - alokrai\\\\nhttps:\\\\/\\\\/arxiv.org\\\\/abs\\\\/2003.11755\\\\n======\\\\nantipaul\\\\nIn a survey on \\\\\"scientific discovery\\\\\", I would have expected more examples\\\\nthan face and image recognition and natural language processing, which are so\\\\nstale at this point.\\\\n\\\\nHealthcare? Physics? Chemistry? Biology? Sociology?\\\\n\\\\n~~~\\\\nanthony_doan\\\\nThe rest you listed require inference and causality.\\\\n\\\\nDeep learning does not do this.\\\\n\\\\nData with less noises are what most deep learning and non statistical models\\\\ndoes well. Meaning that image, nlp, etc.. deep learning does well. But data\\\\nwith lots noises\\\\/uncertainty\\\\/variance or even data that isn\\'t large enough,\\\\nsuch as time series, currently statistical models are still king\\\\n([https:\\\\/\\\\/en.wikipedia.org\\\\/wiki\\\\/Makridakis_Competitions](https:\\\\/\\\\/en.wikipedia.org\\\\/wiki\\\\/Makridakis_Competitions)).\\\\n\\\\nEven with healthcare you\\'re answering a question\\\\/ hypothesis. This is where\\\\nstatistical models strength lies because all statistical models are hypothesis\\\\ntests and vice versa. There are very little opportunity in healthcare where\\\\nyou would use deep learning compare to statistic. I\\'ve seen NLP can be of use\\\\nbut the majority of work in healthcare are inference\\\\/casuality base (this is\\\\nwhy they use propensity model so much). I\\'m in this space public healthcare.\\\\n\\\\n~~~\\\\np1esk\\\\nInteresting you mentioned Makridakis competitions. There\\'s one going on right\\\\nnow on Kaggle, and the current leader believes a NN will be the winning model:\\\\n[https:\\\\/\\\\/www.kaggle.com\\\\/c\\\\/m5-forecasting-\\\\naccuracy\\\\/discussion\\\\/...](https:\\\\/\\\\/www.kaggle.com\\\\/c\\\\/m5-forecasting-\\\\naccuracy\\\\/discussion\\\\/138881)\\\\n\\\\nMore generally, it seems that time series forecasting so far has mostly\\\\nattracted statisticians with little DL experience [1]. Now that there is $50k\\\\nprize, this will be a good test of whether statistical methods are \\\\\"still\\\\nking\\\\\". If I were to enter this field, I\\'d probably look into latest\\\\ntransformer based models, especially the ones used to model raw audio data,\\\\ne.g. [2].\\\\n\\\\nThere\\'s also a real possibility that whenever any strong forecasting method is\\\\ndeveloped (DL based or otherwise) it\\'s not published as the developers simply\\\\nuse it to make money (betting, stock market, etc).\\\\n\\\\n[1]\\\\n[https:\\\\/\\\\/journals.plos.org\\\\/plosone\\\\/article?id=10.1371\\\\/journal...](https:\\\\/\\\\/journals.plos.org\\\\/plosone\\\\/article?id=10.1371\\\\/journal.pone.0194889)\\\\n\\\\n[2] [https:\\\\/\\\\/arxiv.org\\\\/abs\\\\/1904.10509](https:\\\\/\\\\/arxiv.org\\\\/abs\\\\/1904.10509)\\\\n\\\\n~~~\\\\nanthony_doan\\\\nI\\'ll wait to see the result at the end of the competition.\\\\n\\\\nThis is just one of the two competitions for m5. The other one is uncertainty.\\\\n\\\\n------\\\\njefft255\\\\nEric Schmidt, as in Google\\'s ex-CEO, is the second author of this paper! I\\\\ndidn\\'t know he did any scientific research.\\\\n\\\\n~~~\\\\nhervature\\\\nHe has a PhD, unlike Brin and Page.\\\\n\\\\n~~~\\\\njefft255\\\\nRight, but they both were Ph.D. students and Brin I think published quite a\\\\nbit of scientific papers before dropping out.\\\\n\\\\n------\\\\nwswin\\\\nfor the moment I thought it was from 2003\\\\n\\\\n------\\\\nthrowqwerty\\\\nLooks like a good summary. Will read. But at the rate the discipline moves I\\\\nfeel like we need one of these every couple of months for everyone (not just\\\\n\\\\\"lay\\\\\" scientists). Anyone know a good journal or something that produces a\\\\nsimilar sort of survey frequently? Like once a quarter?\\\\n\\\\n~~~\\\\nssivark\\\\n\\\\u201cRate at which the discipline moves\\\\u201d is mostly churn, not progress. Important\\\\ninsights come at a slower rate \\\\u2014 at the speed of human understanding, not at\\\\nthe speed of conference papers. Good papers from even decades ago are likely\\\\nto still be useful \\\\u2014 in fact, they will have the key ideas presented simply\\\\nand clearly, without much jargon or hype. Yes, deep learning practice moves\\\\nquite fast these days, but that\\\\u2019s just the veneer on top of those deeper\\\\nideas, trying out tweaks and variations. That\\\\u2019s not completely an indictment\\\\nof deep learning, rather, any nascent field has a lot of confusing bustle.\\\\n\\\\n------\\\\nbiomodel\\\\nAlways wonder who these kinds of reviews \\\\/ surveys are for? Nobody is going to\\\\nlearn machine learning by reading a 50 page pdf. Meanwhile, people that have\\\\nexperience will have a hard time finding the info they don\\'t already know.\\\\n\\\\nOpinionated & narrow >> Shallow & comprehensive\\\\n\\\\n~~~\\\\nmistrial9\\\\nI will read it, to defend my non-DeepLearning choices for supervised ML .. so\\\\nmany on the bandwagon for unsupervised CNN with their GPUs\\\\n\\\\n~~~\\\\nmistrial9\\\\nI am misunderstood here.. it means, for the purposes that are appropriate, use\\\\na disciplined, supervised model.. and know the strengths and weakness\\' of the\\\\nCNN models.. yes, some reaction to the hype of CNN..\\\\n\\\\n\",\"meta\":\"{\\'id\\': \\'22705028\\'}\"}'\n"
     ]
    }
   ],
   "source": [
    "for line in input_ptr.syft_action_data.iter_lines():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert-success\" style=\"padding:5px;\"><strong>SyftSuccess</strong>: Syft function 'compute_document_data_overlap' successfully created. To add a code request, please create a project using `project = syft.Project(...)`, then use command `project.create_code_request`.</div><br />"
      ],
      "text/plain": [
       "SyftSuccess: Syft function 'compute_document_data_overlap' successfully created. To add a code request, please create a project using `project = syft.Project(...)`, then use command `project.create_code_request`."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@sy.syft_function()\n",
    "def compute_document_data_overlap(document, ngram_index):\n",
    "    from nltk import ngrams\n",
    "    from collections import defaultdict\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    r = re.compile(r\"[\\s{}]+\".format(re.escape(punctuation)))\n",
    "    stats_key_to_input_ids = defaultdict(set)\n",
    "    stats_key_to_reference_ids = defaultdict(set)\n",
    "    document_tokens = r.split(document.lower())\n",
    "    for n in ngram_index.keys():\n",
    "        for document_ngram in ngrams(document_tokens, n):\n",
    "            if document_ngram in ngram_index[n]:\n",
    "                for entry_overlap_key in ngram_index[n][document_ngram]:\n",
    "                    stats_key, id, part = entry_overlap_key.split(\"+\")\n",
    "                    if part == \"input\":\n",
    "                        stats_key_to_input_ids[stats_key].add(id)\n",
    "                    elif part == \"references\":\n",
    "                        stats_key_to_reference_ids[stats_key].add(id)\n",
    "    return stats_key_to_input_ids, stats_key_to_reference_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert-success\" style=\"padding:5px;\"><strong>SyftSuccess</strong>: User Code Submitted</div><br />"
      ],
      "text/plain": [
       "SyftSuccess: User Code Submitted"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.code.submit(compute_document_data_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert-success\" style=\"padding:5px;\"><strong>SyftSuccess</strong>: Syft function 'main_function' successfully created. To add a code request, please create a project using `project = syft.Project(...)`, then use command `project.create_code_request`.</div><br />"
      ],
      "text/plain": [
       "SyftSuccess: Syft function 'main_function' successfully created. To add a code request, please create a project using `project = syft.Project(...)`, then use command `project.create_code_request`."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@sy.syft_function_single_use(input_file=input_ptr, scenario_file=scenario_ptr)\n",
    "def main_function(domain, input_file, scenario_file):\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    import json\n",
    "    from nltk import ngrams\n",
    "    from collections import defaultdict\n",
    "    N = [5, 9, 13]\n",
    "    r = re.compile(r\"[\\s{}]+\".format(re.escape(punctuation)))\n",
    "\n",
    "    def create_ngram_index(light_scenarios, n_values, stats_key_counts):\n",
    "        ngram_index = {n:{}  for n in n_values}\n",
    "        for scenario in light_scenarios:\n",
    "            for n in n_values:\n",
    "                stats_key = scenario['scenario_key'] + '_' + str(n)\n",
    "                stats_key_counts[stats_key] = len(scenario['instances'])\n",
    "                for instance in scenario['instances']:\n",
    "                    id = instance['id']                    \n",
    "                    input_tokens = r.split(instance['input'].lower())\n",
    "                    for input_ngram in ngrams(input_tokens, n):\n",
    "                        if input_ngram not in ngram_index[n]:\n",
    "                            ngram_index[n][input_ngram] = set()\n",
    "                        ngram_index[n][input_ngram].add(stats_key + '+' + id + '+' + 'input')\n",
    "\n",
    "                    # compute reference ngrams\n",
    "                    for reference in instance['references']:\n",
    "                        reference_unigrams = r.split(reference.lower())\n",
    "                        for reference_ngram in ngrams(reference_unigrams, n):\n",
    "                            if reference_ngram not in ngram_index[n]:\n",
    "                                ngram_index[n][reference_ngram] = set()\n",
    "                            ngram_index[n][reference_ngram].add(stats_key + '+' + id + '+' + 'references')\n",
    "        return ngram_index\n",
    "\n",
    "    # # SETUP\n",
    "    light_scenarios = []\n",
    "    light_scenario_jsons = scenario_file.iter_lines()\n",
    "    for light_scenario_json in light_scenario_jsons:\n",
    "        light_scenario_dict: dict = json.loads(light_scenario_json)\n",
    "\n",
    "        light_scenario_key_dict: dict = light_scenario_dict[\"scenario_key\"]\n",
    "        subject_spec = light_scenario_key_dict[\"scenario_spec\"]['args']['subject']\n",
    "        light_scenario_key = subject_spec + '_' + light_scenario_key_dict[\"split\"]\n",
    "        light_instances = [\n",
    "            {\n",
    "                'input': instance_dict['input'], \n",
    "                'references': instance_dict['references'], \n",
    "                'id': instance_dict[\"id\"]\n",
    "            }\n",
    "            for instance_dict in light_scenario_dict[\"instances\"]\n",
    "        ]\n",
    "        light_scenarios.append({'scenario_key': light_scenario_key, 'instances': light_instances})\n",
    "\n",
    "    stats_key_counts = defaultdict(int)\n",
    "    ngram_index = create_ngram_index(\n",
    "        light_scenarios=light_scenarios, n_values=N, stats_key_counts=stats_key_counts\n",
    "    )\n",
    "\n",
    "    # BATCH PROCESSING\n",
    "    jobs = []\n",
    "    for line in input_file.iter_lines():\n",
    "        document = json.loads(line)[\"text\"]\n",
    "        batch_job = domain.launch_job(compute_document_data_overlap,\n",
    "            document=document,\n",
    "            ngram_index=ngram_index,\n",
    "        )\n",
    "        jobs.append(batch_job)\n",
    "    \n",
    "    # AGGREGATION\n",
    "    stats_key_to_input_ids = []\n",
    "    stats_key_to_reference_ids = []\n",
    "    tmp_results = [x.wait() for x in jobs]\n",
    "        \n",
    "    results = [x.get() for x in tmp_results]\n",
    "    for ids, refs in results:\n",
    "        stats_key_to_input_ids.append(ids)\n",
    "        stats_key_to_reference_ids.append(refs)\n",
    "\n",
    "    total_input_ids = defaultdict(set)\n",
    "    total_reference_ids = defaultdict(set)\n",
    "    \n",
    "    for d in stats_key_to_input_ids:\n",
    "        for key in d:\n",
    "            new_set = set()\n",
    "            if key in total_input_ids:\n",
    "                new_set = total_input_ids[key]\n",
    "            new_set = new_set.union(d[key])\n",
    "            total_input_ids[key] = new_set\n",
    "\n",
    "    for d in stats_key_to_reference_ids:\n",
    "        for key in d:\n",
    "            new_set = set()\n",
    "            if key in total_reference_ids:\n",
    "                new_set = total_reference_ids[key]\n",
    "            new_set = total_reference_ids[key].union(d[key])\n",
    "            total_reference_ids[key] = new_set\n",
    "    \n",
    "    all_data_overlap_stats = []\n",
    "    for stats_key, count in stats_key_counts.items():\n",
    "        data_overlap_stats = {\n",
    "            'data_overlap_stats_key': None,\n",
    "            'num_instances': count,\n",
    "            'instance_ids_with_overlapping_input': sorted(total_input_ids[stats_key]),\n",
    "            'instance_ids_with_overlapping_reference': sorted(total_reference_ids[stats_key]),\n",
    "        }\n",
    "        subject, split, n_str = stats_key.split('_')\n",
    "        data_overlap_stats['data_overlap_stats_key'] = {\n",
    "            'light_scenario_key': {'subject': subject, 'split': split},\n",
    "            'overlap_protocol_spec': {'n': int(n_str)}\n",
    "        }\n",
    "        all_data_overlap_stats.append(data_overlap_stats)\n",
    "\n",
    "\n",
    "    return all_data_overlap_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request approved for domain test-domain-helm2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert-success\" style=\"padding:5px;\"><strong>SyftSuccess</strong>: Request 17299686594a4bd9b2e80e9a352fc21b changes applied</div><br />"
      ],
      "text/plain": [
       "SyftSuccess: Request 17299686594a4bd9b2e80e9a352fc21b changes applied"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.code.request_code_execution(main_function)\n",
    "client.requests[-1].approve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.code.main_function(input_file=input_ptr, scenario_file=scenario_ptr, blocking=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "job.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTR OK: True\n",
      "PTR OK: True\n",
      "What: syft.service.request.request.Request\n",
      "Args: {'document': <UID: c7afe443d56f48809331e22d18522865>, 'ngram_index': <UID: b036f5e8c1fb42249743341ab23513c5>}\n",
      "PTR OK: True\n",
      "PTR OK: True\n",
      "What: syft.service.request.request.Request\n",
      "Args: {'document': <UID: 3b03580298314af3b45e1e470e2ed32e>, 'ngram_index': <UID: 0235098138ac4b738ae89b9cacb3dc51>}\n",
      "PTR OK: True\n",
      "PTR OK: True\n",
      "What: syft.service.request.request.Request\n",
      "Args: {'document': <UID: 46745453ed2849e99a876a9dc092e39d>, 'ngram_index': <UID: 1228b4dd25b846c98627d5eac10a4ea7>}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "[{&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;philosophy&#x27;, &#x27;split&#x27;: &#x27;train&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 5}}, &#x27;num_instances&#x27;: 5, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;philosophy&#x27;, &#x27;split&#x27;: &#x27;train&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 9}}, &#x27;num_instances&#x27;: 5, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;philosophy&#x27;, &#x27;split&#x27;: &#x27;train&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 13}}, &#x27;num_instances&#x27;: 5, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;philosophy&#x27;, &#x27;split&#x27;: &#x27;valid&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 5}}, &#x27;num_instances&#x27;: 34, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;philosophy&#x27;, &#x27;split&#x27;: &#x27;valid&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 9}}, &#x27;num_instances&#x27;: 34, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;philosophy&#x27;, &#x27;split&#x27;: &#x27;valid&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 13}}, &#x27;num_instances&#x27;: 34, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;philosophy&#x27;, &#x27;split&#x27;: &#x27;test&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 5}}, &#x27;num_instances&#x27;: 311, &#x27;instance_ids_with_overlapping_input&#x27;: [&#x27;id328&#x27;], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;philosophy&#x27;, &#x27;split&#x27;: &#x27;test&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 9}}, &#x27;num_instances&#x27;: 311, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;philosophy&#x27;, &#x27;split&#x27;: &#x27;test&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 13}}, &#x27;num_instances&#x27;: 311, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;anatomy&#x27;, &#x27;split&#x27;: &#x27;train&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 5}}, &#x27;num_instances&#x27;: 5, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;anatomy&#x27;, &#x27;split&#x27;: &#x27;train&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 9}}, &#x27;num_instances&#x27;: 5, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;anatomy&#x27;, &#x27;split&#x27;: &#x27;train&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 13}}, &#x27;num_instances&#x27;: 5, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;anatomy&#x27;, &#x27;split&#x27;: &#x27;valid&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 5}}, &#x27;num_instances&#x27;: 14, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;anatomy&#x27;, &#x27;split&#x27;: &#x27;valid&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 9}}, &#x27;num_instances&#x27;: 14, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;anatomy&#x27;, &#x27;split&#x27;: &#x27;valid&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 13}}, &#x27;num_instances&#x27;: 14, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;anatomy&#x27;, &#x27;split&#x27;: &#x27;test&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 5}}, &#x27;num_instances&#x27;: 135, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;anatomy&#x27;, &#x27;split&#x27;: &#x27;test&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 9}}, &#x27;num_instances&#x27;: 135, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}, {&#x27;data_overlap_stats_key&#x27;: {&#x27;light_scenario_key&#x27;: {&#x27;subject&#x27;: &#x27;anatomy&#x27;, &#x27;split&#x27;: &#x27;test&#x27;}, &#x27;overlap_protocol_spec&#x27;: {&#x27;n&#x27;: 13}}, &#x27;num_instances&#x27;: 135, &#x27;instance_ids_with_overlapping_input&#x27;: [], &#x27;instance_ids_with_overlapping_reference&#x27;: []}]"
      ],
      "text/plain": [
       "[{'data_overlap_stats_key': {'light_scenario_key': {'subject': 'philosophy',\n",
       "    'split': 'train'},\n",
       "   'overlap_protocol_spec': {'n': 5}},\n",
       "  'num_instances': 5,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'philosophy',\n",
       "    'split': 'train'},\n",
       "   'overlap_protocol_spec': {'n': 9}},\n",
       "  'num_instances': 5,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'philosophy',\n",
       "    'split': 'train'},\n",
       "   'overlap_protocol_spec': {'n': 13}},\n",
       "  'num_instances': 5,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'philosophy',\n",
       "    'split': 'valid'},\n",
       "   'overlap_protocol_spec': {'n': 5}},\n",
       "  'num_instances': 34,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'philosophy',\n",
       "    'split': 'valid'},\n",
       "   'overlap_protocol_spec': {'n': 9}},\n",
       "  'num_instances': 34,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'philosophy',\n",
       "    'split': 'valid'},\n",
       "   'overlap_protocol_spec': {'n': 13}},\n",
       "  'num_instances': 34,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'philosophy',\n",
       "    'split': 'test'},\n",
       "   'overlap_protocol_spec': {'n': 5}},\n",
       "  'num_instances': 311,\n",
       "  'instance_ids_with_overlapping_input': ['id328'],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'philosophy',\n",
       "    'split': 'test'},\n",
       "   'overlap_protocol_spec': {'n': 9}},\n",
       "  'num_instances': 311,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'philosophy',\n",
       "    'split': 'test'},\n",
       "   'overlap_protocol_spec': {'n': 13}},\n",
       "  'num_instances': 311,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'anatomy',\n",
       "    'split': 'train'},\n",
       "   'overlap_protocol_spec': {'n': 5}},\n",
       "  'num_instances': 5,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'anatomy',\n",
       "    'split': 'train'},\n",
       "   'overlap_protocol_spec': {'n': 9}},\n",
       "  'num_instances': 5,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'anatomy',\n",
       "    'split': 'train'},\n",
       "   'overlap_protocol_spec': {'n': 13}},\n",
       "  'num_instances': 5,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'anatomy',\n",
       "    'split': 'valid'},\n",
       "   'overlap_protocol_spec': {'n': 5}},\n",
       "  'num_instances': 14,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'anatomy',\n",
       "    'split': 'valid'},\n",
       "   'overlap_protocol_spec': {'n': 9}},\n",
       "  'num_instances': 14,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'anatomy',\n",
       "    'split': 'valid'},\n",
       "   'overlap_protocol_spec': {'n': 13}},\n",
       "  'num_instances': 14,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'anatomy',\n",
       "    'split': 'test'},\n",
       "   'overlap_protocol_spec': {'n': 5}},\n",
       "  'num_instances': 135,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'anatomy',\n",
       "    'split': 'test'},\n",
       "   'overlap_protocol_spec': {'n': 9}},\n",
       "  'num_instances': 135,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []},\n",
       " {'data_overlap_stats_key': {'light_scenario_key': {'subject': 'anatomy',\n",
       "    'split': 'test'},\n",
       "   'overlap_protocol_spec': {'n': 13}},\n",
       "  'num_instances': 135,\n",
       "  'instance_ids_with_overlapping_input': [],\n",
       "  'instance_ids_with_overlapping_reference': []}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.wait().get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syft_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
