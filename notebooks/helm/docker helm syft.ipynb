{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import os\n",
    "from syft import ActionObject\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a9097",
   "metadata": {},
   "source": [
    "Start this using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32609778-1075-4e3e-8b12-c943165a4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `docker compose --profile blob-storage --file docker-compose.multinode.yml --file docker-compose.dev.yml up`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac03e50-1201-478e-a709-13edeb39f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node = sy.orchestra.launch(name=\"test-domain-1\", port=\"auto\", dev_mode=True, reset=True, node_type=\"domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298dd4c-33aa-4f83-9650-ca4957a1a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = node.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4b5ba-0b1c-4048-b77c-ef9d1c45c60d",
   "metadata": {},
   "source": [
    "`hagrid launch domain to docker:8080 --dev --verbose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = sy.login(url=\"http://localhost:8080\", email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd660e",
   "metadata": {},
   "source": [
    "# Mount storage container with Helm azure container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3475d8-0009-4b4b-9771-fc09ffa7820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELM_STORAGE_ACCOUNT_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.api.services.blob_storage.mount_azure(\n",
    "    account_name='helmprojectstorage',\n",
    "    container_name='helm',\n",
    "    # account_key=os.environ[\"HELM_STORAGE_ACCOUNT_KEY\"],\n",
    "    account_key=HELM_STORAGE_ACCOUNT_KEY,\n",
    "    bucket_name='helmazurebucket',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_files = client.api.services.blob_storage.get_files_from_bucket(bucket_name='helmazurebucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f1f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1a192",
   "metadata": {},
   "source": [
    "# Start workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.worker.start_workers(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35431992-ebaa-4aac-8297-c8df32048ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.worker.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb90c9",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = [f for f in blob_files if \"train-00\" in f.file_name][0]\n",
    "scenario_file = [f for f in blob_files if \"scenario_data\" in f.file_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f466cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "helm_dataset = sy.Dataset(\n",
    "    name=\"Helm Dataset\",\n",
    "    asset_list=[\n",
    "        sy.Asset(\n",
    "            name=\"helm train data\",\n",
    "            data=ActionObject.from_obj([train_file]),\n",
    "            mock=sy.ActionObject.empty()\n",
    "        ),\n",
    "        sy.Asset(\n",
    "            name=\"helm test data\",\n",
    "            data=ActionObject.from_obj([scenario_file]),\n",
    "            mock=sy.ActionObject.empty()\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fe375",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_dataset(helm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "helm_ds = client.datasets[\"Helm Dataset\"]\n",
    "helm_train_files = helm_ds.assets[\"helm train data\"]\n",
    "helm_test_files = helm_ds.assets[\"helm test data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60b056",
   "metadata": {},
   "source": [
    "# Syft functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60223489",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{x+0.12432:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function()\n",
    "def compute_document_data_overlap(scenario_file, input_files, n):\n",
    "    print(\"starting overlap computation\")\n",
    "\n",
    "    from nltk import ngrams\n",
    "    from collections import defaultdict\n",
    "    from string import punctuation\n",
    "    import re, json\n",
    "    import time\n",
    "\n",
    "    r = re.compile(r\"[\\s{}]+\".format(re.escape(punctuation)))\n",
    "    \n",
    "    def create_ngram_index(light_scenarios, n_values, stats_key_counts):\n",
    "        ngram_index = {n:{}  for n in n_values}\n",
    "        for i, scenario in enumerate(light_scenarios):\n",
    "            if i%20 == 0:\n",
    "                print(f\"n_gram indexing progress: {(i/len(light_scenarios))*100:.2f}%\")\n",
    "            for n in n_values:\n",
    "                stats_key = scenario['scenario_key'] + '_' + str(n)\n",
    "                stats_key_counts[stats_key] = len(scenario['instances'])\n",
    "                for instance in scenario['instances']:\n",
    "                    id = instance['id']                    \n",
    "                    input_tokens = r.split(instance['input'].lower())\n",
    "                    for input_ngram in ngrams(input_tokens, n):\n",
    "                        if input_ngram not in ngram_index[n]:\n",
    "                            ngram_index[n][input_ngram] = set()\n",
    "                        ngram_index[n][input_ngram].add(stats_key + '+' + id + '+' + 'input')\n",
    "\n",
    "                    # compute reference ngrams\n",
    "                    for reference in instance['references']:\n",
    "                        reference_unigrams = r.split(reference.lower())\n",
    "                        for reference_ngram in ngrams(reference_unigrams, n):\n",
    "                            if reference_ngram not in ngram_index[n]:\n",
    "                                ngram_index[n][reference_ngram] = set()\n",
    "                            ngram_index[n][reference_ngram].add(stats_key + '+' + id + '+' + 'references')\n",
    "        return ngram_index\n",
    "    \n",
    "    # SETUP\n",
    "    print(\"preparing scenarios and creating indexes\")\n",
    "    start = time.time()\n",
    "    light_scenarios = []\n",
    "    for i, (bytes_read, light_scenario_json) in enumerate(scenario_file.iter_lines(progress=True)):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"scenario creation progress: {(bytes_read/scenario_file.file_size)*100:.2f}%\")\n",
    "\n",
    "        light_scenario_dict: dict = json.loads(light_scenario_json)\n",
    "\n",
    "        light_scenario_key_dict: dict = light_scenario_dict[\"scenario_key\"]\n",
    "        scenario_spec = str(light_scenario_key_dict[\"scenario_spec\"])\n",
    "\n",
    "        light_scenario_key = scenario_spec + '_' + light_scenario_key_dict[\"split\"]\n",
    "        light_instances = [\n",
    "            {\n",
    "                'input': instance_dict['input'], \n",
    "                'references': instance_dict['references'], \n",
    "                'id': instance_dict[\"id\"]\n",
    "            }\n",
    "            for instance_dict in light_scenario_dict[\"instances\"]\n",
    "        ]\n",
    "        light_scenarios.append({'scenario_key': light_scenario_key, 'instances': light_instances})\n",
    "    print(f\"Finished creating scenarios ({time.time()-start}s)\")\n",
    "    \n",
    "    print(\"Creating indexes\")\n",
    "    \n",
    "    start = time.time()\n",
    "    stats_key_counts = defaultdict(int)\n",
    "    ngram_index = create_ngram_index(\n",
    "        light_scenarios=light_scenarios, n_values=[n], stats_key_counts=stats_key_counts\n",
    "    )\n",
    "    print(f\"Finished creating indexes ({time.time()-start}s)\")\n",
    "        \n",
    "    \n",
    "    r = re.compile(r\"[\\s{}]+\".format(re.escape(punctuation)))\n",
    "    stats_key_to_input_ids = defaultdict(set)\n",
    "    stats_key_to_reference_ids = defaultdict(set)\n",
    "    print(\"computing overlap\")\n",
    "    start = time.time()\n",
    "    \n",
    "    for input_file in input_files:\n",
    "        for i, (bytes_read, line) in enumerate(input_file.iter_lines(progress=True)):\n",
    "            if i%1000 == 0:\n",
    "                print(f\"computing overlap progress: {(bytes_read / input_file.file_size) * 100:.2f}%\")            \n",
    "            document = json.loads(line)[\"text\"]\n",
    "            document_tokens = r.split(document.lower())\n",
    "            for n in ngram_index.keys():\n",
    "                for document_ngram in ngrams(document_tokens, n):\n",
    "                    if document_ngram in ngram_index[n]:\n",
    "                        for entry_overlap_key in ngram_index[n][document_ngram]:\n",
    "                            stats_key, id, part = entry_overlap_key.split(\"+\")\n",
    "                            if part == \"input\":\n",
    "                                stats_key_to_input_ids[stats_key].add(id)\n",
    "                            elif part == \"references\":\n",
    "                                stats_key_to_reference_ids[stats_key].add(id)\n",
    "    print(f\"Finished computing overlap ({time.time()-start}s)\")\n",
    "    print(\"done\")\n",
    "    \n",
    "    return stats_key_to_input_ids, stats_key_to_reference_ids, stats_key_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.code.submit(compute_document_data_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function_single_use(input_files=helm_train_files, scenario_files=helm_test_files)\n",
    "def main_function(domain, input_files, scenario_files):\n",
    "    N = [5, 9, 13]\n",
    "    jobs = []\n",
    "    for n in N[:1]:\n",
    "        for scenario_file in scenario_files:\n",
    "            batch_job = domain.launch_job(\n",
    "                compute_document_data_overlap,\n",
    "                scenario_file=scenario_file,\n",
    "                input_files=input_files,\n",
    "                n=n\n",
    "            )\n",
    "            jobs.append(batch_job)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d92df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.code.request_code_execution(main_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee2790",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.requests[-1].approve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b084c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.code.main_function(input_files=helm_train_files,\n",
    "                                scenario_files=helm_test_files,\n",
    "                                blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df60a45",
   "metadata": {},
   "source": [
    "# Inspect Jobs and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job.subjobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3041c-e3d1-4954-9aae-a40e272d5d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d567f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.subjobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63089d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job.wait().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.subjobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852360ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.subjobs[0].logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [j.wait().get() for j in job.subjobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a079df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats_key_to_input_ids, stats_key_to_reference_ids, stats_key_counts\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4daea",
   "metadata": {},
   "source": [
    "# Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5053b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_key_to_input_ids, stats_key_to_reference_ids, stats_key_counts = zip(*results)\n",
    "\n",
    "total_input_ids = defaultdict(set)\n",
    "total_reference_ids = defaultdict(set)\n",
    "total_stats_key_counts = defaultdict(int)\n",
    "\n",
    "for d in stats_key_counts:\n",
    "    for key, val in d.items():\n",
    "        total_stats_key_counts[key] += val\n",
    "\n",
    "\n",
    "for d in stats_key_to_input_ids:\n",
    "    for key in d:\n",
    "        new_set = set()\n",
    "        if key in total_input_ids:\n",
    "            new_set = total_input_ids[key]\n",
    "        new_set = new_set.union(d[key])\n",
    "        total_input_ids[key] = new_set\n",
    "\n",
    "for d in stats_key_to_reference_ids:\n",
    "    for key in d:\n",
    "        new_set = set()\n",
    "        if key in total_reference_ids:\n",
    "            new_set = total_reference_ids[key]\n",
    "        new_set = total_reference_ids[key].union(d[key])\n",
    "        total_reference_ids[key] = new_set\n",
    "\n",
    "all_data_overlap_stats = []\n",
    "for stats_key, count in total_stats_key_counts.items():\n",
    "    data_overlap_stats = {\n",
    "        'data_overlap_stats_key': None,\n",
    "        'num_instances': count,\n",
    "        'instance_ids_with_overlapping_input': sorted(total_input_ids[stats_key]),\n",
    "        'instance_ids_with_overlapping_reference': sorted(total_reference_ids[stats_key]),\n",
    "    }\n",
    "    subject, split, n_str = stats_key.split('_')\n",
    "    data_overlap_stats['data_overlap_stats_key'] = {\n",
    "        'light_scenario_key': {'scenario_spec': subject, 'split': split},\n",
    "        'overlap_protocol_spec': {'n': int(n_str)}\n",
    "    }\n",
    "    all_data_overlap_stats.append(data_overlap_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(all_data_overlap_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300abb87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
