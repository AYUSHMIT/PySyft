{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "\n",
    "# syft absolute\n",
    "import syft as sy\n",
    "from syft import test_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_low = sy.orchestra.launch(\n",
    "    name=\"bigquery-low\",\n",
    "    server_side_type=\"low\",\n",
    "    dev_mode=True,\n",
    "    reset=True,\n",
    "    local_db=True,\n",
    "    n_consumers=1,\n",
    "    create_producer=True,\n",
    ")\n",
    "\n",
    "server_high = sy.orchestra.launch(\n",
    "    name=\"bigquery-high\",\n",
    "    server_side_type=\"high\",\n",
    "    dev_mode=True,\n",
    "    reset=True,\n",
    "    local_db=True,\n",
    "    n_consumers=1,\n",
    "    create_producer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login and launch worker Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_client = server_low.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client = server_high.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(high_client.worker_pools.get_all()) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_worker_pool(client, pool_name):\n",
    "    if pool_name not in [x.name for x in client.worker_pools]:\n",
    "        external_registry = test_settings.get(\"external_registry\", default=\"docker.io\")\n",
    "        worker_docker_tag = f\"openmined/bigquery:{sy.__version__}\"\n",
    "        external_registry, worker_docker_tag\n",
    "        result = client.api.services.worker_image.submit(\n",
    "            worker_config=sy.PrebuiltWorkerConfig(\n",
    "                tag=f\"{external_registry}/{worker_docker_tag}\"\n",
    "            )\n",
    "        )\n",
    "        worker_image = client.images.get_all()[1]\n",
    "        worker_image\n",
    "        result = client.api.services.image_registry.add(external_registry)\n",
    "        result = client.api.services.worker_pool.launch(\n",
    "            pool_name=pool_name,\n",
    "            image_uid=worker_image.id,\n",
    "            num_workers=1,\n",
    "        )\n",
    "        result\n",
    "        return result\n",
    "    else:\n",
    "        print(\"Already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_name = \"bigquery-pool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_worker_pool(high_client, pool_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_worker_pool(low_client, pool_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = high_client.worker_pools.scale(number=5, pool_name=worker_pool_name)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(high_client.worker_pools.get_all()) == 2\n",
    "assert len(low_client.worker_pools.get_all()) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_worker_image = high_client.images.get_all()[0]\n",
    "base_worker_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_client.register(\n",
    "    email=\"data_scientist@openmined.org\",\n",
    "    password=\"verysecurepassword\",\n",
    "    password_verify=\"verysecurepassword\",\n",
    "    name=\"John Doe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.settings.allow_guest_signup(enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(low_client.api.services.user.get_all()) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worker_dockerfile = f\"\"\"\n",
    "# FROM {str(base_worker_image.image_identifier)}\n",
    "\n",
    "# RUN uv pip install db-dtypes google-cloud-bigquery\n",
    "\n",
    "# \"\"\".strip()\n",
    "# worker_dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker_tag = str(base_worker_image.image_identifier).replace(\n",
    "#     \"backend\", \"worker-bigquery\"\n",
    "# )\n",
    "# docker_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.api_endpoint_method(\n",
    "    settings={\n",
    "        \"credentials\": test_settings.gce_service_account.to_dict(),\n",
    "        \"region\": test_settings.gce_region,\n",
    "        \"project_id\": test_settings.gce_project_id,\n",
    "    }\n",
    ")\n",
    "def private_query_function(\n",
    "    context,\n",
    "    sql_query: str,\n",
    ") -> str:\n",
    "    # third party\n",
    "    from google.cloud import bigquery  # noqa: F811\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    # syft absolute\n",
    "    from syft import SyftException\n",
    "\n",
    "    # Auth for Bigquer based on the workload identity\n",
    "    credentials = service_account.Credentials.from_service_account_info(\n",
    "        context.settings[\"credentials\"]\n",
    "    )\n",
    "    scoped_credentials = credentials.with_scopes(\n",
    "        [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "    )\n",
    "\n",
    "    client = bigquery.Client(\n",
    "        credentials=scoped_credentials,\n",
    "        location=context.settings[\"region\"],\n",
    "    )\n",
    "\n",
    "    # third party\n",
    "\n",
    "    # Auth for Bigquer based on the workload identity\n",
    "    try:\n",
    "        rows = client.query_and_wait(\n",
    "            sql_query,\n",
    "            project=context.settings[\"project_id\"],\n",
    "        )\n",
    "\n",
    "        if rows.total_rows > 1_000_000:\n",
    "            raise SyftException(\n",
    "                public_message=\"Please only write queries that gather aggregate statistics\"\n",
    "            )\n",
    "\n",
    "        return rows.to_dataframe()\n",
    "    except Exception as e:\n",
    "        # We MUST handle the errors that we want to be visible to the data owners.\n",
    "        # Any exception not catched is visible only to the data owner.\n",
    "        # not a bigquery exception\n",
    "        if not hasattr(e, \"_errors\"):\n",
    "            output = f\"got exception e: {type(e)} {str(e)}\"\n",
    "            raise SyftException(\n",
    "                public_message=f\"An error occured executing the API call {output}\"\n",
    "            )\n",
    "\n",
    "        if e._errors[0][\"reason\"] in [\n",
    "            \"badRequest\",\n",
    "            \"blocked\",\n",
    "            \"duplicate\",\n",
    "            \"invalidQuery\",\n",
    "            \"invalid\",\n",
    "            \"jobBackendError\",\n",
    "            \"jobInternalError\",\n",
    "            \"notFound\",\n",
    "            \"notImplemented\",\n",
    "            \"rateLimitExceeded\",\n",
    "            \"resourceInUse\",\n",
    "            \"resourcesExceeded\",\n",
    "            \"tableUnavailable\",\n",
    "            \"timeout\",\n",
    "        ]:\n",
    "            raise SyftException(\n",
    "                public_message=\"Error occured during the call: \"\n",
    "                + e._errors[0][\"message\"]\n",
    "            )\n",
    "        else:\n",
    "            raise SyftException(\n",
    "                public_message=\"An error occured executing the API call, please contact the domain owner.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any helper methods for our rate limiter\n",
    "def is_within_rate_limit(context):\n",
    "    \"\"\"Rate limiter for custom API calls made by users.\"\"\"\n",
    "    # stdlib\n",
    "    import datetime\n",
    "\n",
    "    state = context.state\n",
    "    settings = context.settings\n",
    "    email = context.user.email\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    calls_last_min = [\n",
    "        1 if (current_time - call_time).seconds < 60 else 0\n",
    "        for call_time in state[email]\n",
    "    ]\n",
    "\n",
    "    return sum(calls_last_min) < settings[\"CALLS_PER_MIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mock endpoint that the researchers can use for testing\n",
    "\n",
    "\n",
    "@sy.api_endpoint_method(\n",
    "    settings={\n",
    "        \"credentials\": test_settings.gce_service_account.to_dict(),\n",
    "        \"region\": test_settings.gce_region,\n",
    "        \"project_id\": test_settings.gce_project_id,\n",
    "        \"CALLS_PER_MIN\": 10,\n",
    "    },\n",
    "    helper_functions=[is_within_rate_limit],\n",
    ")\n",
    "def mock_query_function(\n",
    "    context,\n",
    "    sql_query: str,\n",
    ") -> str:\n",
    "    # stdlib\n",
    "    import datetime\n",
    "\n",
    "    # third party\n",
    "    from google.cloud import bigquery  # noqa: F811\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    # syft absolute\n",
    "    from syft import SyftException\n",
    "\n",
    "    # Auth for Bigquer based on the workload identity\n",
    "    credentials = service_account.Credentials.from_service_account_info(\n",
    "        context.settings[\"credentials\"]\n",
    "    )\n",
    "    scoped_credentials = credentials.with_scopes(\n",
    "        [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "    )\n",
    "\n",
    "    client = bigquery.Client(\n",
    "        credentials=scoped_credentials,\n",
    "        location=context.settings[\"region\"],\n",
    "    )\n",
    "\n",
    "    # Store a dict with the calltimes for each user, via the email.\n",
    "    if context.user.email not in context.state.keys():\n",
    "        context.state[context.user.email] = []\n",
    "\n",
    "    if not context.code.is_within_rate_limit(context):\n",
    "        raise SyftException(\n",
    "            public_message=\"Rate limit of calls per minute has been reached.\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        context.state[context.user.email].append(datetime.datetime.now())\n",
    "\n",
    "        rows = client.query_and_wait(\n",
    "            sql_query,\n",
    "            project=context.settings[\"project_id\"],\n",
    "        )\n",
    "\n",
    "        if rows.total_rows > 1_000_000:\n",
    "            raise SyftException(\n",
    "                public_message=\"Please only write queries that gather aggregate statistics\"\n",
    "            )\n",
    "\n",
    "        return rows.to_dataframe()\n",
    "\n",
    "    except Exception as e:\n",
    "        # not a bigquery exception\n",
    "        if not hasattr(e, \"_errors\"):\n",
    "            output = f\"got exception e: {type(e)} {str(e)}\"\n",
    "            raise SyftException(\n",
    "                public_message=f\"An error occured executing the API call {output}\"\n",
    "            )\n",
    "\n",
    "        # Treat all errors that we would like to be forwarded to the data scientists\n",
    "        # By default, any exception is only visible to the data owner.\n",
    "\n",
    "        if e._errors[0][\"reason\"] in [\n",
    "            \"badRequest\",\n",
    "            \"blocked\",\n",
    "            \"duplicate\",\n",
    "            \"invalidQuery\",\n",
    "            \"invalid\",\n",
    "            \"jobBackendError\",\n",
    "            \"jobInternalError\",\n",
    "            \"notFound\",\n",
    "            \"notImplemented\",\n",
    "            \"rateLimitExceeded\",\n",
    "            \"resourceInUse\",\n",
    "            \"resourcesExceeded\",\n",
    "            \"tableUnavailable\",\n",
    "            \"timeout\",\n",
    "        ]:\n",
    "            raise SyftException(\n",
    "                public_message=\"Error occured during the call: \"\n",
    "                + e._errors[0][\"message\"]\n",
    "            )\n",
    "        else:\n",
    "            raise SyftException(\n",
    "                public_message=\"An error occured executing the API call, please contact the domain owner.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_endpoint = sy.TwinAPIEndpoint(\n",
    "    path=\"bigquery.test_query\",\n",
    "    description=\"This endpoint allows to query Bigquery storage via SQL queries.\",\n",
    "    private_function=private_query_function,\n",
    "    mock_function=mock_query_function,\n",
    "    worker_pool=pool_name,\n",
    ")\n",
    "\n",
    "high_client.custom_api.add(endpoint=new_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we update the endpoint to timeout after 100s (rather the default of 60s)\n",
    "high_client.api.services.api.update(\n",
    "    endpoint_path=\"bigquery.test_query\", endpoint_timeout=120\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.api.services.api.update(\n",
    "    endpoint_path=\"bigquery.test_query\", hide_mock_definition=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mock version\n",
    "result = high_client.api.services.bigquery.test_query.mock(\n",
    "    sql_query=f\"SELECT * FROM {test_settings.dataset_1}.{test_settings.table_1} LIMIT 10\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.api_endpoint(\n",
    "    path=\"bigquery.schema\",\n",
    "    description=\"This endpoint allows for visualising the metadata of tables available in BigQuery.\",\n",
    "    settings={\n",
    "        \"credentials\": test_settings.gce_service_account.to_dict(),\n",
    "        \"region\": test_settings.gce_region,\n",
    "        \"project_id\": test_settings.gce_project_id,\n",
    "        \"dataset_1\": test_settings.dataset_1,\n",
    "        \"table_1\": test_settings.table_1,\n",
    "        \"table_2\": test_settings.table_2,\n",
    "        \"CALLS_PER_MIN\": 5,\n",
    "    },\n",
    "    helper_functions=[\n",
    "        is_within_rate_limit\n",
    "    ],  # Adds ratelimit as this is also a method available to data scientists\n",
    "    worker_pool=pool_name,\n",
    ")\n",
    "def schema_function(\n",
    "    context,\n",
    ") -> str:\n",
    "    # stdlib\n",
    "    import datetime\n",
    "\n",
    "    # third party\n",
    "    from google.cloud import bigquery  # noqa: F811\n",
    "    from google.oauth2 import service_account\n",
    "    import pandas as pd\n",
    "\n",
    "    # syft absolute\n",
    "    from syft import SyftException\n",
    "\n",
    "    # Auth for Bigquer based on the workload identity\n",
    "    credentials = service_account.Credentials.from_service_account_info(\n",
    "        context.settings[\"credentials\"]\n",
    "    )\n",
    "    scoped_credentials = credentials.with_scopes(\n",
    "        [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "    )\n",
    "\n",
    "    client = bigquery.Client(\n",
    "        credentials=scoped_credentials,\n",
    "        location=context.settings[\"region\"],\n",
    "    )\n",
    "\n",
    "    if context.user.email not in context.state.keys():\n",
    "        context.state[context.user.email] = []\n",
    "\n",
    "    if not context.code.is_within_rate_limit(context):\n",
    "        raise SyftException(\n",
    "            public_message=\"Rate limit of calls per minute has been reached.\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        context.state[context.user.email].append(datetime.datetime.now())\n",
    "\n",
    "        # Formats the data schema in a data frame format\n",
    "        # Warning: the only supported format types are primitives, np.ndarrays and pd.DataFrames\n",
    "\n",
    "        data_schema = []\n",
    "        for table_id in [\n",
    "            f\"{context.settings[\"dataset_1\"]}.{context.settings[\"table_1\"]}\",\n",
    "            f\"{context.settings[\"dataset_1\"]}.{context.settings[\"table_2\"]}\",\n",
    "        ]:\n",
    "            table = client.get_table(table_id)\n",
    "            for schema in table.schema:\n",
    "                data_schema.append(\n",
    "                    {\n",
    "                        \"project\": str(table.project),\n",
    "                        \"dataset_id\": str(table.dataset_id),\n",
    "                        \"table_id\": str(table.table_id),\n",
    "                        \"schema_name\": str(schema.name),\n",
    "                        \"schema_field\": str(schema.field_type),\n",
    "                        \"description\": str(table.description),\n",
    "                        \"num_rows\": str(table.num_rows),\n",
    "                    }\n",
    "                )\n",
    "        return pd.DataFrame(data_schema)\n",
    "\n",
    "    except Exception as e:\n",
    "        # not a bigquery exception\n",
    "        if not hasattr(e, \"_errors\"):\n",
    "            output = f\"got exception e: {type(e)} {str(e)}\"\n",
    "            raise SyftException(\n",
    "                public_message=f\"An error occured executing the API call {output}\"\n",
    "            )\n",
    "\n",
    "        # Should add appropriate error handling for what should be exposed to the data scientists.\n",
    "        raise SyftException(\n",
    "            public_message=\"An error occured executing the API call, please contact the domain owner.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.custom_api.add(endpoint=schema_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.api.services.bigquery.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.api_endpoint(\n",
    "    path=\"bigquery.submit_query\",\n",
    "    description=\"API endpoint that allows you to submit SQL queries to run on the private data.\",\n",
    "    worker_pool=pool_name,\n",
    "    settings={\"worker\": pool_name},\n",
    ")\n",
    "def submit_query(\n",
    "    context,\n",
    "    func_name: str,\n",
    "    query: str,\n",
    ") -> str:\n",
    "    # stdlib\n",
    "    import hashlib\n",
    "\n",
    "    # syft absolute\n",
    "    import syft as sy\n",
    "\n",
    "    hash_object = hashlib.new(\"sha256\")\n",
    "\n",
    "    hash_object.update(context.user.email.encode(\"utf-8\"))\n",
    "    func_name = func_name + \"_\" + hash_object.hexdigest()[:6]\n",
    "\n",
    "    @sy.syft_function(\n",
    "        name=func_name,\n",
    "        input_policy=sy.MixedInputPolicy(\n",
    "            endpoint=sy.Constant(\n",
    "                val=context.admin_client.api.services.bigquery.test_query\n",
    "            ),\n",
    "            query=sy.Constant(val=query),\n",
    "            client=context.admin_client,\n",
    "        ),\n",
    "        worker_pool_name=context.settings[\"worker\"],\n",
    "    )\n",
    "    def execute_query(query: str, endpoint):\n",
    "        res = endpoint(sql_query=query)\n",
    "        return res\n",
    "\n",
    "    request = context.user_client.code.request_code_execution(execute_query)\n",
    "    context.admin_client.requests.set_tags(request, [\"autosync\"])\n",
    "\n",
    "    return (\n",
    "        f\"Query submitted {request}. Use `client.code.{func_name}()` to run your query\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.custom_api.add(endpoint=submit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.api.services.api.update(\n",
    "    endpoint_path=\"bigquery.submit_query\", hide_mock_definition=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.custom_api.api_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(high_client.custom_api.api_endpoints()) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    high_client.api.services.bigquery.test_query\n",
    "    and high_client.api.services.bigquery.submit_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mock version\n",
    "result = high_client.api.services.bigquery.test_query.mock(\n",
    "    sql_query=f\"SELECT * FROM {test_settings.dataset_1}.{test_settings.table_1} LIMIT 10\"\n",
    ")\n",
    "assert len(result) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo can we clean up the duplicate exception messages?\n",
    "\n",
    "# Test mock version for wrong queries\n",
    "with sy.raises(\n",
    "    sy.SyftException(public_message=\"*must be qualified with a dataset*\"), show=True\n",
    "):\n",
    "    high_client.api.services.bigquery.test_query.mock(\n",
    "        sql_query=\"SELECT * FROM invalid_table LIMIT 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test private version\n",
    "result = high_client.api.services.bigquery.test_query.private(\n",
    "    sql_query=f\"SELECT * FROM {test_settings.dataset_1}.{test_settings.table_1} LIMIT 1\"\n",
    ")\n",
    "result\n",
    "\n",
    "assert len(result) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = sy.sync(from_client=high_client, to_client=low_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.click_sync(0)\n",
    "widget.click_sync(1)\n",
    "widget.click_sync(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low side research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(low_client.custom_api.api_endpoints()) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = low_client.api.services.bigquery.test_query.mock(\n",
    "    sql_query=\"SELECT * from data_10gb.comments limit 10\"\n",
    ")\n",
    "assert len(result) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sy.raises(sy.SyftException, show=True):\n",
    "    low_client.api.services.bigquery.test_query.private(\n",
    "        sql_query=\"SELECT * from data_10gb.comments limit 10\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = low_client.api.services.bigquery.schema()\n",
    "# third party\n",
    "import pandas as pd\n",
    "\n",
    "assert isinstance(res.get(), pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNC_NAME = \"large_sample\"\n",
    "LARGE_SAMPLE_QUERY = (\n",
    "    f\"SELECT * FROM {test_settings.dataset_2}.{test_settings.table_2} LIMIT 10000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_res = low_client.api.services.bigquery.test_query(sql_query=LARGE_SAMPLE_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = low_client.api.services.bigquery.submit_query(\n",
    "    func_name=FUNC_NAME, query=LARGE_SAMPLE_QUERY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_path(response):\n",
    "    # stdlib\n",
    "    import re\n",
    "\n",
    "    pattern = r\"client\\.code\\.(\\w+)\\(\\)\"\n",
    "    match = re.search(pattern, str(response))\n",
    "    if match:\n",
    "        extracted_code = match.group(1)\n",
    "        return extracted_code\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why are we randomizing things here?\n",
    "func_name = extract_code_path(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_method = getattr(low_client.code, func_name, None)\n",
    "api_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: this is very noisy, but it actually passes\n",
    "with sy.raises(\n",
    "    sy.SyftException(\n",
    "        public_message=\"*Please wait for the admin to allow the execution of this code*\"\n",
    "    ),\n",
    "    show=True,\n",
    "):\n",
    "    result = api_method(blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync, approve, sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: this is way too noisy\n",
    "widget = sy.sync(from_client=low_client, to_client=high_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not great\n",
    "widget.click_sync(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(high_client.code.get_all()) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = high_client.requests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syft absolute\n",
    "from syft.service.code.user_code import UserCode\n",
    "from syft.service.request.request import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_request(client, request) -> dict:\n",
    "    if not isinstance(request, Request):\n",
    "        return \"This is not a request\"\n",
    "\n",
    "    code = request.code\n",
    "    if not isinstance(code, UserCode):\n",
    "        return \"No usercode found\"\n",
    "\n",
    "    func_name = request.code.service_func_name\n",
    "    api_func = getattr(client.code, func_name, None)\n",
    "    if api_func is None:\n",
    "        return \"Code name was not found on the client.\"\n",
    "\n",
    "    job = api_func(blocking=False)\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = execute_request(high_client, request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "from sync_helpers import sync_finished_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_finished_jobs(client_low=low_client, client_high=high_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_client.requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS: Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = api_method(blocking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = job.wait().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(res, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(res) == 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNC_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_high.land()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo \"$worker_dockerfile\" | docker build -t $docker_tag -q -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker image ls | grep bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
